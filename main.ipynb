{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import socket\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reads the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset as panda dataframe, skip last rows with text\n",
    "data = pd.read_fwf(\"../Data/data.txt\")\n",
    "data = data.iloc[:-4] #Removes the summary lines\n",
    "data = data.loc[:, ~data.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column processing (needed sometimes from the .txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split IP address and port to two columns, and drops the old column\n",
    "data[['Src IP Addr', 'Src Port']] = data['Src IP Addr:Port'].str.split(':', n=1, expand=True)\n",
    "data[['Dst IP Addr', 'Dst Port']] = data['Dst IP Addr:Port'].str.split(':', n=1, expand=True)\n",
    "#data[['Bytes', 'Flows']] = data['Bytes Flows'].str.split(' ', n=1, expand=True)\n",
    "#data[['Duration', 'Proto']] = data['Duration Proto'].str.split(' ', n=1, expand=True)\n",
    "#data = data.drop(columns=[\"Src IP Addr:Port\", \"Dst IP Addr:Port\", \"Bytes Flows\", \"Flows\", 'Duration Proto'])\n",
    "data = data.drop(columns=[\"Src IP Addr:Port\", \"Dst IP Addr:Port\", \"Flows\"])\n",
    "data = data.rename(columns={\"seen\": \"Time\", \"Date first\": \"Date\"})\n",
    "data['Src Port'] = data['Src Port'].str.extract(r'(\\d+)')\n",
    "data['Src Port'] = pd.to_numeric(data['Src Port'], errors='coerce')\n",
    "#print(data)\n",
    "\n",
    "## Split IP address and port to two columns, and drops the old column (OLD)\n",
    "# data[['Src IP Addr', 'Src Port']] = data['Src IP Addr:Port'].str.split(':', n=1, expand=True)\n",
    "# data[['Dst IP Addr', 'Dst Port']] = data['Dst IP Addr:Port'].str.split(':', n=1, expand=True)\n",
    "# data = data.drop(columns=[\"Src IP Addr:Port\", \"Dst IP Addr:Port\",\"Flows\"])\n",
    "# data = data.rename(columns={\"seen\": \"Time\", \"Date first\": \"Date\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bytes(value):\n",
    "    multipliers = {'K': 1000, 'M': 1000000, 'G': 1000000000}\n",
    "\n",
    "    # Split the value into numerical part and prefix (if present)\n",
    "    parts = value.split()\n",
    "    num_part = float(parts[0]) #if parts[0].isdigit() else None\n",
    "    prefix = parts[1] if len(parts) > 1 else None\n",
    "\n",
    "    # Check if a valid prefix is present\n",
    "    if prefix and prefix in multipliers:\n",
    "        return num_part * multipliers[prefix]\n",
    "    elif num_part is not None:\n",
    "        # If no valid prefix is found but there is a numerical part, return it as is\n",
    "        return num_part\n",
    "    else:\n",
    "        # If neither numerical part nor valid prefix is found, return the original value\n",
    "        return float(value)\n",
    "\n",
    "# Apply the conversion function to the 'Bytes' column\n",
    "data['Bytes'] = data['Bytes'].apply(convert_bytes)\n",
    "\n",
    "# Create column with global seconds\n",
    "def create_seconds_column(df):\n",
    "\n",
    "    def create_datetime(date_str, time_str):\n",
    "        date_obj = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "        time_obj = datetime.strptime(time_str, \"%H:%M:%S.%f\")\n",
    "        datetime_obj = date_obj + timedelta(hours = time_obj.hour, minutes=time_obj.minute, seconds = time_obj.second, microseconds=time_obj.microsecond)\n",
    "        return datetime_obj\n",
    "    \n",
    "    def seconds_diff(dt_obj, first_time):\n",
    "        return((dt_obj-first_time).total_seconds())\n",
    "\n",
    "\n",
    "\n",
    "    # first_time = create_datetime(df.iloc[0][\"Date\"], df.iloc[0][\"Time\"])\n",
    "    first_time = min([create_datetime(date, time) for date,time in zip(df[\"Date\"], df[\"Time\"])])\n",
    "    \n",
    "    df[\"Seconds\"] = df.apply(lambda row: seconds_diff(create_datetime(row[\"Date\"], row[\"Time\"]), first_time=first_time), axis = 1)\n",
    "\n",
    "create_seconds_column(data)\n",
    "\n",
    "# Orders the data by time in ascending order\n",
    "data = data.sort_values(by='Seconds', ascending=True)\n",
    "\n",
    "# Reset the index\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Changes Duration column to float\n",
    "data['Duration'] = data['Duration'].astype(float)\n",
    "\n",
    "# Relation between duration and bytes\n",
    "# data['Rate'] = data['Bytes']/data['Duration'].replace(0, pd.NaT)\n",
    "\n",
    "# Find clients IP address\n",
    "client = data['Src IP Addr'].value_counts().idxmax()\n",
    "\n",
    "# Initialize 'Host IP'-column from 'Src Ip Addr'\n",
    "data['Host IP'] = data['Src IP Addr']\n",
    "\n",
    "# If the destination IP is not equal to the clients IP, adds it to 'Host IP'-column\n",
    "for index, row in data.iterrows():\n",
    "    if row['Dst IP Addr'] != client:\n",
    "        data.at[index, 'Host IP'] = row['Dst IP Addr']\n",
    "\n",
    "# Find all unique addresses\n",
    "unique_ip = data['Host IP'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a new dataframe\n",
    "data_DNS = pd.DataFrame(columns=['IP', 'Host'])\n",
    "\n",
    "data_DNS['IP']=unique_ip\n",
    "host = []\n",
    "\n",
    "# for-loop for doing reverse DNS lookup\n",
    "i=0\n",
    "for ip in unique_ip:\n",
    "    try:\n",
    "        host_name = socket.gethostbyaddr(ip)[0]\n",
    "        host.append(host_name)\n",
    "    except socket.herror:\n",
    "        host.append(None)\n",
    "    \n",
    "    if i % 100 == 0: # Used to keep track how far along we've come\n",
    "        print(f\"{i} / {len(unique_ip)}\")\n",
    "    i += 1\n",
    "\n",
    "# Adds the corresponding domain names to the IP-addresses and creates a CSV-file\n",
    "data_DNS['Host'] = host\n",
    "data_DNS.to_csv('./host_names', index=False)\n",
    "    \n",
    "print(data_DNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a 'Group' column with NaN values\n",
    "data['Group'] = float('nan')\n",
    "\n",
    "# Gives the flows happening close to each other the same group number\n",
    "group_counter = 0\n",
    "t = 0.5 # Specify how close in time\n",
    "for i in range(len(data) - 1):\n",
    "    if data.iloc[i + 1]['Seconds'] - data.iloc[i]['Seconds'] <= t:\n",
    "        if data.iloc[i]['Seconds'] - data.iloc[i-1]['Seconds'] > t:# If this is a new grouping, increase group_counter by one\n",
    "            group_counter += 1\n",
    "        data.at[i + 1, 'Group'] = group_counter\n",
    "        data.at[i, 'Group'] = group_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changes Protocol into feature-columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dummies and store it in a variable\n",
    "dummies = pd.get_dummies(data.Proto).astype(int)\n",
    " \n",
    "# Concatenate the dummies to original dataframe\n",
    "data = pd.concat([data, dummies], axis='columns')\n",
    "\n",
    "# drop the values\n",
    "data = data.drop(['Proto', 'Src Port', \"Dst Port\"], axis='columns')\n",
    "data = data.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Makes all HOST ports into feature-columns (1 or 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds clients IP and only saves the first 3 octets\n",
    "client = unique_ip.split('.')\n",
    "client = '.'.join(client[:3])\n",
    "\n",
    "# Creates a column for non-client ports\n",
    "data['Host Port'] = None\n",
    "\n",
    "# Adds the non-client port to the new column by checking that the ports (Src & Dst) does not contain the client IP\n",
    "for index, row in data.iterrows():\n",
    "    if client not in row['Src IP Addr']:\n",
    "        data.at[index, 'Host Port'] = row['Src Port']\n",
    "    elif client not in row['Dst IP Addr']:\n",
    "        data.at[index, 'Host Port'] = row['Dst Port'] \n",
    "\n",
    "# Checks all unique ports used\n",
    "unique_port = data['Host Port'].unique()\n",
    "unique_port = unique_port[unique_port != None]\n",
    "\n",
    "# Adds the unique ports as columns and initiates all to False\n",
    "data[unique_port] = 0\n",
    "\n",
    "# Checks every host port and adds a True statement to the correct column\n",
    "nr_port = len(unique_port)\n",
    "for col in data.columns[-nr_port:]:\n",
    "    data[col] = (data['Host Port'] == int(col)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes outliers and data labled \"Unknown\"\n",
    "data = data.drop(data[data.Label == \"Unknown\"].index)\n",
    "data = data.drop(data[data.Duration > 20000].index)\n",
    "\n",
    "#bag-words url:sen\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X = vectorizer.fit_transform(data['Domain_Name'])\n",
    "\n",
    "df_bow = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Identify columns with only numeric names\n",
    "numeric_columns = df_bow.columns[df_bow.columns.str.isnumeric()]\n",
    "\n",
    "# Drop columns with only numeric names\n",
    "df_bow.drop(numeric_columns, axis=1, inplace=True)\n",
    "\n",
    "# print(len(data.index))\n",
    "# print(len(df_bow.index))\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "df_bow.reset_index(drop=True, inplace=True)\n",
    "data = pd.concat([data, df_bow], axis=1)\n",
    "# data = data.drop('Domain_Name', axis=1)\n",
    "\n",
    "\n",
    "# Drop the large labled data point\n",
    "# data = data.drop(data[data.Label == \"Netflix\"].index)\n",
    "# data = data.drop(data[data.Label == \"Youtube\"].index)\n",
    "# data = data.drop(data[data.Label == \"Facebook\"].index)\n",
    "# data = data.drop(data[data.Label == \"YouTube\"].index)\n",
    "# data = data.drop(data[data.Label == \"Gmail\"].index)\n",
    "# data = data.drop(data[data.Label == \"Instagram\"].index)\n",
    "# data = data.drop(data[data.Label == \"X\"].index)\n",
    "# data = data.drop(data[data.Label == \"Outlook Mail\"].index)\n",
    "# data = data.drop(data[data.Label == \"Steam Gaming\"].index)\n",
    "# data = data.drop(data[data.Label == \"Reddit\"].index)\n",
    "# data = data.drop(data[data.Label == \"SVT Play\"].index)\n",
    "# data = data.drop(data[data.Label == \"Twitch TV\"].index)\n",
    "# data = data.drop(data[data.Label == \"Google Drive\"].index)\n",
    "# data = data.drop(data[data.Label == \"Prime video\"].index)\n",
    "# data = data.drop(data[data.Label == \"Amazon shopping\"].index)\n",
    "# data = data.drop(data[data.Label == \"Google Drive\"].index)\n",
    "\n",
    "\n",
    "# Normalize data\n",
    "\n",
    "columns = ['Bytes', 'Duration', 'Packets']\n",
    "for column in columns:\n",
    "    data[column] = (data[column] - data[column].min()) / (data[column].max() - data[column].min()) \n",
    "\n",
    "\n",
    "\n",
    "# Separate data into train and test data\n",
    "col_drop = [\"Date\", \"Time\", \"Src_IP_Addr\", \"Dst_IP_Addr\", \"Host_IP\", \"Rate\", \"Domain_Name\", 'Label'] # Adblocker\n",
    "msk = np.random.rand(len(data)) < 0.8\n",
    "train = data[msk]\n",
    "test = data[~msk]\n",
    "\n",
    "# Creates training X- and Y-data\n",
    "Xtrain = train.drop(columns=col_drop)\n",
    "Ytrain = train[\"Label\"]\n",
    "\n",
    "# Creates test X- and Y-data\n",
    "Xtest = test.drop(columns=col_drop)\n",
    "Ytest = test[\"Label\"]\n",
    "\n",
    "# print(Xtest)\n",
    "# print(train)\n",
    "# print(np.size(train))\n",
    "# print(np.size(test))\n",
    "\n",
    "# NN-model using lbfgs \n",
    "n = data[\"Label\"].value_counts()\n",
    "print(n)\n",
    "\n",
    "clf = MLPClassifier(solver='adam',alpha=1e-5, hidden_layer_sizes=(10,10,15), random_state=1)\n",
    "\n",
    "model = clf.fit(Xtrain, Ytrain)\n",
    "\n",
    "Yhat = model.predict(Xtest)\n",
    "\n",
    "# print(Yhat)\n",
    "# print(Ytest)\n",
    "\n",
    "def accuracy(yhat,ytest):\n",
    "    print(yhat)\n",
    "    print(ytest)\n",
    "    count = 0\n",
    "    for ind, _ in enumerate(yhat):\n",
    "        if yhat[ind] == ytest[ind]:\n",
    "            count += 1\n",
    "    \n",
    "    print(count/len(ytest))\n",
    "\n",
    "\n",
    "accuracy(Yhat,np.array(Ytest))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
